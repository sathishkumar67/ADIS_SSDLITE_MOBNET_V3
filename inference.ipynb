{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3320c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the ADIS repository\n",
    "!git clone https://github.com/sathishkumar67/SSD_MobileNetV3_ADIS.git\n",
    "# move the files to the current directory\n",
    "!mv /kaggle/working/SSD_MobileNetV3_ADIS/* /kaggle/working/\n",
    "# upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# install the required packages\n",
    "!pip install  -r requirements.txt --upgrade --upgrade-strategy eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9871aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from typing import List\n",
    "import os\n",
    "import optuna\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from ssdlite_mobnetv3_adis.utils import unzip_file, replace_activation_function\n",
    "from ssdlite_mobnetv3_adis.dataset import collate_fn, SSDLITEOBJDET_DATASET, CachedSSDLITEOBJDET_DATASET\n",
    "from ssdlite_mobnetv3_adis.model import SSDLITE_MOBILENET_V3_Large\n",
    "from ssdlite_mobnetv3_adis.trainer import bohb_tunner, train\n",
    "\n",
    "# set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd607711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89529a6b53e45d0ad1c57cb25751e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "balanced_dataset.zip:   0%|          | 0.00/7.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping: 100%|██████████| 7.07G/7.07G [00:41<00:00, 168MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped /kaggle/working/balanced_dataset.zip to /kaggle/working\n",
      "Removed zip file: /kaggle/working/balanced_dataset.zip\n",
      "Removed zip file: /kaggle/working/balanced_dataset.zip\n"
     ]
    }
   ],
   "source": [
    "# set constants\n",
    "REPO_ID = \"pt-sk/ADIS\" \n",
    "DATASET_NAME = \"balanced_dataset\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "FILENAME_IN_REPO = f\"{DATASET_NAME}.zip\"\n",
    "LOCAL_DIR = os.getcwd()\n",
    "DATASET_PATH = f\"{LOCAL_DIR}/{FILENAME_IN_REPO}\"\n",
    "DATASET_FOLDER_PATH = f\"{LOCAL_DIR}/{DATASET_NAME}\"                       \n",
    "CLASSES = ['Cat', 'Cattle', 'Chicken', 'Deer', 'Dog', 'Squirrel', 'Eagle', 'Goat', 'Rodents', 'Snake'] \n",
    "NUM_CLASSES = len(CLASSES)\n",
    "NUM_CLASSES_WITH_BG = NUM_CLASSES + 1    # 1 for background class\n",
    "\n",
    "# download the dataset and unzip it\n",
    "hf_hub_download(repo_id=REPO_ID, filename=FILENAME_IN_REPO, repo_type=REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "unzip_file(DATASET_PATH, LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db86b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset and caching to /kaggle/working/balanced_dataset/train_cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9536/18139 [02:07<06:02, 23.72it/s] libpng warning: iCCP: known incorrect sRGB profile\n",
      " 53%|█████▎    | 9550/18139 [02:08<03:23, 42.16it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|██████████| 18139/18139 [03:42<00:00, 81.34it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset and caching to /kaggle/working/balanced_dataset/val_cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390/2390 [00:25<00:00, 92.03it/s] \n",
      "100%|██████████| 2390/2390 [00:25<00:00, 92.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset and caching to /kaggle/working/balanced_dataset/test_cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 1757/2390 [00:20<00:08, 78.71it/s] libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|██████████| 2390/2390 [00:27<00:00, 86.36it/s] \n",
      "100%|██████████| 2390/2390 [00:27<00:00, 86.36it/s] \n"
     ]
    }
   ],
   "source": [
    "# set pin memory device\n",
    "PIN_MEMORY_DEVICE = \"cuda:0\"\n",
    "NUM_CORES = os.cpu_count()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# prepare the dataset\n",
    "train_dataset = CachedSSDLITEOBJDET_DATASET(\n",
    "    dataset_class=SSDLITEOBJDET_DATASET,\n",
    "    root_dir=DATASET_FOLDER_PATH,\n",
    "    split=\"train\",\n",
    "    num_classes=NUM_CLASSES_WITH_BG)\n",
    "\n",
    "val_dataset = CachedSSDLITEOBJDET_DATASET(\n",
    "    dataset_class=SSDLITEOBJDET_DATASET,\n",
    "    root_dir=DATASET_FOLDER_PATH,\n",
    "    split=\"val\",\n",
    "    num_classes=NUM_CLASSES_WITH_BG)\n",
    "\n",
    "test_dataset = CachedSSDLITEOBJDET_DATASET(\n",
    "    dataset_class=SSDLITEOBJDET_DATASET,\n",
    "    root_dir=DATASET_FOLDER_PATH,\n",
    "    split=\"test\",\n",
    "    num_classes=NUM_CLASSES_WITH_BG)\n",
    "\n",
    "\n",
    "# samplers for reproducibility\n",
    "train_sampler = RandomSampler(train_dataset, generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "val_sampler = RandomSampler(val_dataset, generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "test_sampler = RandomSampler(test_dataset, generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "\n",
    "\n",
    "# prepare the dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=NUM_CORES,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory_device=PIN_MEMORY_DEVICE)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=val_sampler,\n",
    "    num_workers=NUM_CORES,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory_device=PIN_MEMORY_DEVICE)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=test_sampler,\n",
    "    num_workers=NUM_CORES,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory_device=PIN_MEMORY_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7117bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ckpt = torch.load(\"/kaggle/working/ckpt/ssdlite_mobv3_custom_params_ckpt.pth\", map_location=\"cpu\")\n",
    "best_ckpt = torch.load(\"/kaggle/working/ckpt/ssdlite_mobnetv3_bestparams_ckpt.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abbd0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\" to /root/.cache/torch/hub/checkpoints/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\n",
      "100%|██████████| 13.4M/13.4M [00:02<00:00, 4.97MB/s]\n",
      "100%|██████████| 13.4M/13.4M [00:02<00:00, 4.97MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = SSDLITE_MOBILENET_V3_Large(num_classes_with_bg=NUM_CLASSES_WITH_BG)\n",
    "custom_model.load_state_dict(custom_ckpt['model_state_dict'], strict=True)\n",
    "custom_model.to(device)\n",
    "custom_model.eval()\n",
    "\n",
    "best_model = SSDLITE_MOBILENET_V3_Large(num_classes_with_bg=NUM_CLASSES_WITH_BG)\n",
    "best_model.load_state_dict(best_ckpt['model_state_dict'], strict=True)\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46a73df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb39296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# df_metrics = calculate_per_class_with_iou(best_model, train_loader, device, classes=CLASSES)\n",
    "# df_metrics.loc[\"Average\"] = df_metrics.mean()\n",
    "# print(f\"Per-class metrics for test set:\\n{df_metrics}\")\n",
    "# end_time = time.time()\n",
    "# print(f\"Time taken for test set evaluation: {end_time - start_time:.2f} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10d25518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute per class metrics for validation set\n",
    "# compute_per_class_metrics(best_model, val_loader, device, class_names=CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "523d2d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a959a39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_average_metrics() got multiple values for argument 'dataset_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76/390841616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_average_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ValidationSet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: compute_average_metrics() got multiple values for argument 'dataset_name'"
     ]
    }
   ],
   "source": [
    "compute_average_metrics(best_model, val_loader, device, dataset_name=\"ValidationSet\", class_names=CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from torchvision.ops import box_iou\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# compute and print average metrics for a model and dataloader\n",
    "def compute_average_metrics(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    class_names: list,\n",
    "    conf_thresh: float = 0.2,\n",
    "    iou_thresh: float = 0.5\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes per-class metrics (accuracy, precision, recall, f1-score, IoU, mAP@50, mAP@50:95) for a model and dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model for evaluation.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for evaluation.\n",
    "        device (torch.device): Device to run model on.\n",
    "        class_names (list): List of class names for DataFrame index.\n",
    "        conf_thresh (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n",
    "        iou_thresh (float, optional): IoU threshold for matching. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with per-class metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"[Stage 1] Initializing metric containers and TorchMetrics...\")\n",
    "\n",
    "    counters = defaultdict(lambda: {\"tp\":0,\"fp\":0,\"fn\":0,\"support\":0})\n",
    "    iou_sums = defaultdict(float)\n",
    "    iou_counts = defaultdict(int)\n",
    "\n",
    "    metric = MeanAveragePrecision(\n",
    "        box_format='xyxy',\n",
    "        iou_type='bbox',\n",
    "        iou_thresholds=[0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95],\n",
    "        class_metrics=True,\n",
    "        extended_summary=True\n",
    "    )\n",
    "\n",
    "    print(\"[Stage 2] Starting inference and metric collection...\")\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader, desc=\"Inference\", leave=True):\n",
    "\n",
    "            # Move images to device\n",
    "            images = images.to(device) if isinstance(images, torch.Tensor) else [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            preds = []\n",
    "            targs = []\n",
    "\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "\n",
    "                # Prepare TorchMetrics format\n",
    "                preds.append({\n",
    "                    'boxes': out['boxes'].cpu(),\n",
    "                    'scores': out['scores'].cpu(),\n",
    "                    'labels': out['labels'].cpu()\n",
    "                })\n",
    "                targs.append({\n",
    "                    'boxes': tgt['boxes'].cpu(),\n",
    "                    'labels': tgt['labels'].cpu()\n",
    "                })\n",
    "\n",
    "                # Per-class metrics\n",
    "                pred_boxes = out[\"boxes\"].cpu()\n",
    "                pred_scores = out[\"scores\"].cpu()\n",
    "                pred_labels = out[\"labels\"].cpu()\n",
    "                true_boxes = tgt[\"boxes\"]\n",
    "                true_labels = tgt[\"labels\"]\n",
    "\n",
    "                # Filter by confidence\n",
    "                keep = pred_scores > conf_thresh\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_labels = pred_labels[keep]\n",
    "\n",
    "                # Count support (number of GT instances per class)\n",
    "                for lbl in true_labels.tolist():\n",
    "                    counters[lbl][\"support\"] += 1\n",
    "\n",
    "                # No predictions → all GT are FN\n",
    "                if pred_boxes.numel() == 0:\n",
    "                    for lbl in true_labels.tolist():\n",
    "                        counters[lbl][\"fn\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # Compute IoU matrix and find matches\n",
    "                iou_matrix = box_iou(pred_boxes, true_boxes)\n",
    "                matches = torch.nonzero(iou_matrix > iou_thresh, as_tuple=False)\n",
    "                matched_pred, matched_true = set(), set()\n",
    "\n",
    "                for pi, ti in matches.tolist():\n",
    "                    matched_pred.add(pi)\n",
    "                    matched_true.add(ti)\n",
    "                    p_lbl = int(pred_labels[pi].item())\n",
    "                    t_lbl = int(true_labels[ti].item())\n",
    "                    if p_lbl == t_lbl:\n",
    "                        counters[p_lbl][\"tp\"] += 1\n",
    "                        iou_sums[p_lbl] += iou_matrix[pi, ti].item()\n",
    "                        iou_counts[p_lbl] += 1\n",
    "                    else:\n",
    "                        counters[p_lbl][\"fp\"] += 1\n",
    "                        counters[t_lbl][\"fn\"] += 1\n",
    "\n",
    "                # Unmatched → FP or FN\n",
    "                for pi in range(len(pred_boxes)):\n",
    "                    if pi not in matched_pred:\n",
    "                        cls = int(pred_labels[pi].item())\n",
    "                        counters[cls][\"fp\"] += 1\n",
    "                for ti in range(len(true_boxes)):\n",
    "                    if ti not in matched_true:\n",
    "                        cls = int(true_labels[ti].item())\n",
    "                        counters[cls][\"fn\"] += 1\n",
    "\n",
    "            metric.update(preds, targs)\n",
    "\n",
    "    print(\"[Stage 3] Computing per-class metrics and building DataFrame...\")\n",
    "\n",
    "    results = {}\n",
    "    for cls, cnt in counters.items():\n",
    "        tp, fp, fn, sup = cnt[\"tp\"], cnt[\"fp\"], cnt[\"fn\"], cnt[\"support\"]\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "        avg_iou = iou_sums[cls]/iou_counts[cls] if iou_counts[cls]>0 else 0.0\n",
    "        accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0\n",
    "        results[cls] = {\n",
    "            \"count\": sup,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"avg_iou\": avg_iou\n",
    "        }\n",
    "\n",
    "    df_metrics = pd.DataFrame(results).T\n",
    "\n",
    "    # Set class names as index\n",
    "    df_metrics.index = [class_names[idx-1] for idx in df_metrics.index]\n",
    "    df_metrics = df_metrics.sort_index()\n",
    "\n",
    "    # Add mAP columns from TorchMetrics\n",
    "    tm_results = metric.compute()\n",
    "    if \"map_50\" in tm_results:\n",
    "        df_metrics[\"mAP@50\"] = tm_results[\"map_50\"].cpu().tolist()\n",
    "    if \"map\" in tm_results:\n",
    "        df_metrics[\"mAP@[50:95]\"] = tm_results[\"map\"].cpu().tolist()\n",
    "\n",
    "    # Add last row for average metrics\n",
    "    df_metrics.loc[\"Average\"] = df_metrics.mean()\n",
    "\n",
    "    print(\"[Stage 4] Average Metrics:\")\n",
    "    print(df_metrics.T[\"Average\"])\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[Stage 5] Evaluation completed. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678dcc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
